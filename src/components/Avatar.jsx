/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/64f1a714fe61576b46f27ca2.glb -o src/components/Avatar.jsx -k -r public
*/

import { useAnimations, useGLTF } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { button, useControls } from "leva";
import React, { useEffect, useMemo, useRef, useState } from "react";

import * as THREE from "three";
import { useChat } from "../hooks/useChat";

const facialExpressions = {
  default: {},
  smile: {
    browInnerUp: 0.17,
    eyeSquintLeft: 0.4,
    eyeSquintRight: 0.44,
    noseSneerLeft: 0.1700000727403593,
    noseSneerRight: 0.14000002836874015,
    mouthPressLeft: 0.61,
    mouthPressRight: 0.41000000000000003,
  },
  funnyFace: {
    jawLeft: 0.63,
    mouthPucker: 0.53,
    noseSneerLeft: 1,
    noseSneerRight: 0.39,
    mouthLeft: 1,
    eyeLookUpLeft: 1,
    eyeLookUpRight: 1,
    cheekPuff: 0.9999924982764238,
    mouthDimpleLeft: 0.414743888682652,
    mouthRollLower: 0.32,
    mouthSmileLeft: 0.35499733688813034,
    mouthSmileRight: 0.35499733688813034,
  },
  sad: {
    mouthFrownLeft: 1,
    mouthFrownRight: 1,
    mouthShrugLower: 0.78341,
    browInnerUp: 0.452,
    eyeSquintLeft: 0.72,
    eyeSquintRight: 0.75,
    eyeLookDownLeft: 0.5,
    eyeLookDownRight: 0.5,
    jawForward: 1,
  },
  surprised: {
    eyeWideLeft: 0.5,
    eyeWideRight: 0.5,
    jawOpen: 0.351,
    mouthFunnel: 1,
    browInnerUp: 1,
  },
  angry: {
    browDownLeft: 1,
    browDownRight: 1,
    eyeSquintLeft: 1,
    eyeSquintRight: 1,
    jawForward: 1,
    jawLeft: 1,
    mouthShrugLower: 1,
    noseSneerLeft: 1,
    noseSneerRight: 0.42,
    eyeLookDownLeft: 0.16,
    eyeLookDownRight: 0.16,
    cheekSquintLeft: 1,
    cheekSquintRight: 1,
    mouthClose: 0.23,
    mouthFunnel: 0.63,
    mouthDimpleRight: 1,
  },
  crazy: {
    browInnerUp: 0.9,
    jawForward: 1,
    noseSneerLeft: 0.5700000000000001,
    noseSneerRight: 0.51,
    eyeLookDownLeft: 0.39435766259644545,
    eyeLookUpRight: 0.4039761421719682,
    eyeLookInLeft: 0.9618479575523053,
    eyeLookInRight: 0.9618479575523053,
    jawOpen: 0.9618479575523053,
    mouthDimpleLeft: 0.9618479575523053,
    mouthDimpleRight: 0.9618479575523053,
    mouthStretchLeft: 0.27893590769016857,
    mouthStretchRight: 0.2885543872656917,
    mouthSmileLeft: 0.5578718153803371,
    mouthSmileRight: 0.38473918302092225,
    tongueOut: 0.9618479575523053,
  },
};

const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_AA",
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
  I: "viseme_I",
  O: "viseme_O",
  U: "viseme_U",
};

let setupMode = false;

export function Avatar(props) {
  const { nodes, materials, scene, animations: modelAnimations } = useGLTF(
    "/models/64f1a714fe61576b46f27ca2.glb",
    undefined,
    (error) => {
      console.warn("Error loading main model:", error);
    }
  );

  const { message, onMessagePlayed, currentAudio, currentAnalyser } = useChat();
  const [animation, setAnimation] = useState("");

  // --- Force arms down on initial load ---
  useEffect(() => {
    if (!nodes?.Wolf3D_Head) return;
    // These bone names are typical for Wolf3D avatars; adjust if needed
    const leftUpperArm = nodes?.LeftArm;
    const rightUpperArm = nodes?.RightArm;
    const leftLowerArm = nodes?.LeftForeArm;
    const rightLowerArm = nodes?.RightForeArm;
    
    // Only set initial arm positions if no animation is running AND we're not in talking mode
    const isTalking = animation && animation.includes('Talk');
    if ((!animation || animation === "Idle") && !isTalking) {
      console.log("Setting initial arm positions");
      // Bring arms much closer and more naturally to body
      if (rightUpperArm) {
        leftUpperArm.rotation.z = 0.1; // arms straight down
        leftUpperArm.rotation.y = 0.0;
        leftUpperArm.rotation.x = 1.15; // natural shoulder drop
      }
      if (leftUpperArm) {
        rightUpperArm.rotation.z = 0.0;
        rightUpperArm.rotation.y = 0.0;
        rightUpperArm.rotation.x = 1.25;
      }
      if (leftLowerArm) leftLowerArm.rotation.z = -0.0;
      if (rightLowerArm) rightLowerArm.rotation.z = 0.0;
    } else if (isTalking) {
      console.log("Skipping initial arm positioning during talking animation");
    }
  }, [nodes, animation]);

  const [lipsync, setLipsync] = useState();

  const { animations } = useGLTF("/models/animations.glb", undefined, (error) => {
    console.warn("Error loading animations:", error);
  });

  // Check if required data is loaded
  const isModelLoaded = nodes && materials && scene;
  const areAnimationsLoaded = animations && animations.length > 0;

  // Filter out animations that reference missing bones
  const filteredAnimations = useMemo(() => {
    if (!areAnimationsLoaded || !scene) return [];
    
    // Get all bone names from the current scene
    const boneNames = new Set();
    scene.traverse((child) => {
      if (child.isBone) {
        boneNames.add(child.name);
      }
    });
    
    // Log all bone names for debugging
    console.log("All available bones:", Array.from(boneNames));
    
    // Specifically check for arm/hand bones
    const armHandBones = Array.from(boneNames).filter(name => 
      name.toLowerCase().includes('arm') || 
      name.toLowerCase().includes('hand') ||
      name.toLowerCase().includes('forearm') ||
      name.toLowerCase().includes('shoulder') ||
      name.toLowerCase().includes('wrist') ||
      name.toLowerCase().includes('finger')
    );
    
    console.log("Arm/Hand bones found:", armHandBones);
    
    // Check if we have arm/hand bones
    const hasArmHandBones = armHandBones.length > 0;
    console.log("Model has arm/hand bones:", hasArmHandBones);
    
    // Filter animations to only include those with tracks for existing bones
    return animations.map(animation => {
      const validTracks = animation.tracks.filter(track => {
        // Extract bone name from track path (e.g., "HeadTop_End_end.position" -> "HeadTop_End_end")
        const boneName = track.name.split('.')[0];
        const hasBone = boneNames.has(boneName);
        
        // Debug log for arm/hand bones
        if (boneName.toLowerCase().includes('arm') || boneName.toLowerCase().includes('hand')) {
          console.log(`Arm/Hand bone check - ${boneName}: ${hasBone ? 'FOUND' : 'MISSING'}`);
        }
        
        return hasBone;
      });
      
      // Log if we're filtering out arm/hand tracks
      const originalTrackCount = animation.tracks.length;
      const filteredTrackCount = validTracks.length;
      
      if (originalTrackCount !== filteredTrackCount) {
        console.log(`Animation ${animation.name} had ${originalTrackCount} tracks, now ${filteredTrackCount}`);
        
        // Check specifically for arm/hand tracks that were filtered out
        const armHandTracks = animation.tracks.filter(track => {
          const boneName = track.name.split('.')[0];
          return boneName.toLowerCase().includes('arm') || boneName.toLowerCase().includes('hand') ||
                 boneName.toLowerCase().includes('forearm') || boneName.toLowerCase().includes('shoulder') ||
                 boneName.toLowerCase().includes('wrist') || boneName.toLowerCase().includes('finger');
        });
        
        const validArmHandTracks = validTracks.filter(track => {
          const boneName = track.name.split('.')[0];
          return boneName.toLowerCase().includes('arm') || boneName.toLowerCase().includes('hand') ||
                 boneName.toLowerCase().includes('forearm') || boneName.toLowerCase().includes('shoulder') ||
                 boneName.toLowerCase().includes('wrist') || boneName.toLowerCase().includes('finger');
        });
        
        if (armHandTracks.length > validArmHandTracks.length) {
          console.warn(`WARNING: ${armHandTracks.length - validArmHandTracks.length} arm/hand tracks were filtered out of ${animation.name}`);
          console.log("Missing arm/hand bones:", armHandTracks
            .filter(track => !validTracks.includes(track))
            .map(track => track.name.split('.')[0]));
        }
      }
      
      return {
        ...animation,
        tracks: validTracks
      };
    }).filter(animation => animation.tracks.length > 0); // Only include animations with valid tracks
  }, [animations, scene, areAnimationsLoaded]);

  const group = useRef();
  const { actions, mixer } = useAnimations(isModelLoaded && areAnimationsLoaded ? filteredAnimations : [], group);

  useEffect(() => {
    console.log("Message effect triggered", message);
    if (!message) {
      // When there's no message, set to idle animation
      console.log("No message, setting to idle");
      setAnimation("Idle");
      setFacialExpression("");
      setLipsync(undefined);
      return;
    }
    console.log("Setting animation from message:", message.animation);
    
    // Set animation based on whether there's audio (speaking) or not
    if (message.audio) {
      // Use talking animation when there's audio to play
      // Prefer animations that likely have hand movements
      const availableTalkingAnimations = filteredAnimations.filter(a => 
        a.name.toLowerCase().includes('talk') || 
        a.name.toLowerCase().includes('speak')
      );
      
      // Prefer animations with hand movements
      const talkingWithHands = availableTalkingAnimations.filter(a => {
        const hasArmHandTracks = a.tracks?.some(track => 
          track.name.toLowerCase().includes('arm') || 
          track.name.toLowerCase().includes('hand')
        ) || false;
        
        console.log(`Checking animation ${a.name} for arm/hand tracks:`, {
          trackCount: a.tracks?.length,
          hasArmHandTracks,
          trackNames: a.tracks?.map(t => t.name).slice(0, 10) // First 10 track names
        });
        
        return hasArmHandTracks;
      });
      
      // Select the best talking animation
      const talkAnimation = message.animation || 
                           (talkingWithHands.length > 0 ? talkingWithHands[0].name : null) ||
                           (availableTalkingAnimations.length > 0 ? availableTalkingAnimations[0].name : null) ||
                           "Talking_0";
                            
      console.log("Setting talking animation:", talkAnimation, {
        messageAnimation: message.animation,
        hasTalkingWithHands: talkingWithHands.length > 0,
        hasAvailableTalking: availableTalkingAnimations.length > 0,
        talkingWithHandsNames: talkingWithHands.map(a => a.name),
        availableTalkingNames: availableTalkingAnimations.map(a => a.name),
        allFilteredAnimations: filteredAnimations.map(a => ({
          name: a.name,
          trackCount: a.tracks?.length,
          hasArmTracks: a.tracks?.some(track => 
            track.name.toLowerCase().includes('arm') || 
            track.name.toLowerCase().includes('hand'))
        }))
      });
      setAnimation(talkAnimation);
      
      // Log message details for debugging
      console.log("Message details:", {
        hasAudio: !!message.audio,
        audioLength: message.audio?.length,
        text: message.text,
        animation: message.animation
      });
    } else {
      // Use idle animation when there's no audio
      console.log("Setting idle animation");
      setAnimation("Idle");
    }
    
    setFacialExpression(message.facialExpression);
    setLipsync(message.lipsync);
  }, [message, filteredAnimations]);

  useEffect(() => {
    if (!isModelLoaded || !areAnimationsLoaded || filteredAnimations.length === 0) {
      return;
    }
    
    // Log available animations for debugging
    console.log("Available animations:", filteredAnimations.map(a => ({
      name: a.name,
      duration: a.duration,
      tracks: a.tracks?.length || 0
    })));
    
    // Look for gesture or hand-specific animations
    const gestureAnimations = filteredAnimations.filter(a => 
      a.name.toLowerCase().includes('gesture') || 
      a.name.toLowerCase().includes('hand') || 
      a.name.toLowerCase().includes('wave') ||
      a.name.toLowerCase().includes('point') ||
      a.name.toLowerCase().includes('talk') // Include talking animations as they should have hand movements
    );
    
    // Look for talking animations specifically
    const talkingAnimations = filteredAnimations.filter(a => 
      a.name.toLowerCase().includes('talk') || 
      a.name.toLowerCase().includes('speak')
    );
    
    // Look for Talking_0 animation
    const talking0Anim = filteredAnimations.find(a => a.name === "Talking_0");
    console.log("Found Talking_0 animation:", !!talking0Anim, talking0Anim?.name);
    
    // Look for talking animations that have hand movements
    const talkingWithHands = talkingAnimations.filter(a => {
      // Check if the animation has arm/hand tracks
      return a.tracks?.some(track => 
        track.name.toLowerCase().includes('arm') || 
        track.name.toLowerCase().includes('hand')
      ) || false;
    });
    
    console.log("Talking animations:", talkingAnimations.map(a => a.name));
    console.log("Talking animations with hand tracks:", talkingWithHands.map(a => a.name));
    
    // Log details of Talking_0 animation if it exists
    if (talking0Anim) {
      const talking0HasArmHandTracks = talking0Anim.tracks?.some(track => 
        track.name.toLowerCase().includes('arm') || 
        track.name.toLowerCase().includes('hand') ||
        track.name.toLowerCase().includes('forearm') ||
        track.name.toLowerCase().includes('shoulder') ||
        track.name.toLowerCase().includes('wrist') ||
        track.name.toLowerCase().includes('finger')
      ) || false;
      
      console.log("Talking_0 animation details:", {
        name: talking0Anim.name,
        duration: talking0Anim.duration,
        tracks: talking0Anim.tracks?.length,
        trackNames: talking0Anim.tracks?.map(t => t.name.substring(0, t.name.indexOf('.'))),
        hasArmHandTracks: talking0HasArmHandTracks,
        // Check specifically for arm/hand tracks
        armTracks: talking0Anim.tracks?.filter(t => 
          t.name.toLowerCase().includes('arm') || 
          t.name.toLowerCase().includes('hand') || 
          t.name.toLowerCase().includes('forearm') ||
          t.name.toLowerCase().includes('shoulder') ||
          t.name.toLowerCase().includes('wrist') ||
          t.name.toLowerCase().includes('finger')
        ).map(t => t.name)
      });
    }
    
    // Only set default animation if none is set
    if (!animation) {
      // Prefer animations in this order:
      // 1. Talking_0 (if it has hand movements)
      // 2. Other talking animations with hand movements
      // 3. Talking_0 (fallback)
      // 4. Other talking animations
      // 5. Gesture animations (explicitly for hand movements)
      // 6. Idle (fallback)
      // 7. First available
      
      const talking0HasHands = talking0Anim && talking0Anim.tracks?.some(track => 
        track.name.toLowerCase().includes('arm') || 
        track.name.toLowerCase().includes('hand') ||
        track.name.toLowerCase().includes('forearm') ||
        track.name.toLowerCase().includes('shoulder') ||
        track.name.toLowerCase().includes('wrist') ||
        track.name.toLowerCase().includes('finger')
      );
      
      console.log("Talking_0 has hand movements:", talking0HasHands);
      
      const newAnimation = (talking0HasHands && talking0Anim)
        ? "Talking_0"
        : talkingWithHands.length > 0
        ? talkingWithHands[0].name
        : filteredAnimations.find((a) => a.name === "Talking_0")
        ? "Talking_0"
        : talkingAnimations.length > 0
        ? talkingAnimations[0].name
        : gestureAnimations.length > 0
        ? gestureAnimations[0].name
        : filteredAnimations.find((a) => a.name === "Idle")
        ? "Idle"
        : filteredAnimations[0]?.name || "";
      
      console.log("Selected initial animation:", newAnimation, {
        talking0HasHands,
        hasTalkingWithHands: talkingWithHands.length > 0,
        hasTalkingAnimations: talkingAnimations.length > 0,
        hasGestureAnimations: gestureAnimations.length > 0,
        hasIdle: !!filteredAnimations.find((a) => a.name === "Idle"),
        firstAnimation: filteredAnimations[0]?.name || "None"
      });
      setAnimation(newAnimation);
    }
  }, [filteredAnimations, isModelLoaded, areAnimationsLoaded]);

  useEffect(() => {
    console.log("Animation effect triggered", { animation, hasActions: !!actions, actionExists: actions[animation] });
    
    if (!animation || !actions[animation]) {
      // If no animation is set, try to play idle animation
      if (actions["Idle"]) {
        console.log("Playing idle animation");
        actions["Idle"].reset().fadeIn(0.5).play();
      }
      return;
    }
    
    console.log("Playing animation:", animation);
    
    try {
      // Fade out any currently playing animation
      Object.keys(actions).forEach(actionName => {
        if (actions[actionName].isRunning() && actionName !== animation) {
          console.log("Fading out animation:", actionName);
          actions[actionName].fadeOut(0.3);
        }
      });
      
      // Play the new animation
      console.log("Starting animation:", animation);
      const action = actions[animation];
      
      // Debug: Log animation details
      console.log("Animation details:", {
        name: animation,
        hasAction: !!action,
        isRunning: action?.isRunning(),
        timeScale: action?.timeScale,
        weight: action?.getEffectiveWeight(),
        clipDuration: action?.getClip()?.duration,
        clipTracks: action?.getClip()?.tracks?.length
      });
      
      // For talking animations, increase the weight for more pronounced movements
      if (animation.toLowerCase().includes('talk') || animation.toLowerCase().includes('gesture')) {
        action.setEffectiveWeight(1.5); // Increase weight for more visible movements
        action.timeScale = 1.2; // Slightly increase speed for more noticeable movements
        console.log("Set increased weight and speed for talking animation");
      } else {
        action.setEffectiveWeight(1.0); // Normal weight
        action.timeScale = 1.0; // Normal speed
        console.log("Set normal weight and speed for animation");
      }
      
      action
        .reset()
        .fadeIn(0.3)
        .play();
      
      console.log("Animation started successfully");
        
      // Return cleanup function to fade out when component unmounts or animation changes
      return () => {
        if (action && action.isRunning()) {
          console.log("Cleaning up animation:", animation);
          action.fadeOut(0.3);
        }
      };
    } catch (error) {
      console.warn("Error playing animation:", error);
      console.error("Full error details:", error.stack);
      
      // Fallback: If animation fails, try to at least move the hands manually during speech
      if (animation.toLowerCase().includes('talk')) {
        console.log("Fallback: Will use manual hand movements during talking");
      }
    }
  }, [animation, actions, mixer.stats.actions.inUse]);

  const lerpMorphTarget = (target, value, speed = 0.1) => {
    if (!isModelLoaded) return;
    
    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (
          index === undefined ||
          child.morphTargetInfluences[index] === undefined
        ) {
          return;
        }
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );

        // Removed Leva control updates to prevent errors
        // Leva controls should not be updated individually like this
      }
    });
  };

  const [blink, setBlink] = useState(false);
  const [winkLeft, setWinkLeft] = useState(false);
  const [winkRight, setWinkRight] = useState(false);
  const [facialExpression, setFacialExpression] = useState("");
  const audioEnergyRef = useRef(0);
  
  useFrame(() => {
    // Only run animation logic if model is loaded
    if (!isModelLoaded || !areAnimationsLoaded) return;
    
    // Debug: Log animation status periodically
    if (Math.floor(Date.now() / 1000) % 5 === 0) { // Every 5 seconds
      if (animation && actions[animation]?.isRunning()) {
        console.log("Animation status:", {
          name: animation,
          time: actions[animation]?.time,
          duration: actions[animation]?.getClip()?.duration
        });
      } else if (animation) {
        console.log("Animation selected but not running:", animation, "Action exists:", !!actions[animation]);
      }
    }
    
    // Debug: Log arm positions periodically to see if they're moving
    if (Math.floor(Date.now() / 1000) % 3 === 0) { // Every 3 seconds
      if (nodes?.LeftArm && nodes?.RightArm) {
        console.log("Arm positions:", {
          leftArmRotation: nodes.LeftArm.rotation.toArray(),
          rightArmRotation: nodes.RightArm.rotation.toArray()
        });
      } else {
        console.log("Arm nodes not found. Available nodes:", Object.keys(nodes || {}));
      }
    }
    
    !setupMode &&
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        const mapping = facialExpressions[facialExpression];
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return; // eyes wink/blink are handled separately
        }
        if (mapping && mapping[key]) {
          lerpMorphTarget(key, mapping[key], 0.1);
        } else {
          lerpMorphTarget(key, 0, 0.1);
        }
      });

    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.5);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.5);

    // PROFESSIONAL LIP-SYNC WITH COARTICULATION
    if (setupMode || !message || !lipsync || !currentAudio) {
      // Reset all morph targets when not in use
      Object.values(corresponding).forEach((value) => {
        lerpMorphTarget(value, 0, 0.15);
      });
      return;
    }

    // Update real-time audio amplitude from Web Audio analyser
    if (currentAnalyser?.analyser && currentAnalyser?.dataArray) {
      try {
        currentAnalyser.analyser.getByteTimeDomainData(currentAnalyser.dataArray);
        let sumSquares = 0;
        for (let i = 0; i < currentAnalyser.dataArray.length; i++) {
          const value = (currentAnalyser.dataArray[i] - 128) / 128;
          sumSquares += value * value;
        }
        const rms = Math.sqrt(sumSquares / currentAnalyser.dataArray.length);
        const normalized = Math.min(rms * 3, 1); // Boost energy slightly
        audioEnergyRef.current = audioEnergyRef.current * 0.7 + normalized * 0.3;
      } catch (error) {
        if (process.env.NODE_ENV === 'development') {
          console.warn('Audio analyser processing error:', error);
        }
      }
    } else {
      audioEnergyRef.current *= 0.92;
    }

    const audioEnergy = audioEnergyRef.current;

    const currentAudioTime = currentAudio.currentTime;
    const audioDuration = currentAudio.duration || 1;
    const lastCueEnd = lipsync?.duration && lipsync.duration > 0
      ? lipsync.duration
      : (lipsync.mouthCues?.length ? lipsync.mouthCues[lipsync.mouthCues.length - 1].end || 0 : 0);
    const durationScale = lastCueEnd > 0 ? audioDuration / lastCueEnd : 1;
    
    // Track applied morph targets for smooth transitions
    const appliedMorphTargets = new Set();
    
    // Process mouth cues with coarticulation and anticipatory timing
    if (lipsync.mouthCues && lipsync.mouthCues.length > 0) {
      for (let i = 0; i < lipsync.mouthCues.length; i++) {
        const mouthCue = lipsync.mouthCues[i];
        const cueStart = (mouthCue.start ?? 0) * durationScale;
        const cueEnd = (mouthCue.end ?? 0) * durationScale;
        const effectiveStart = Math.max(cueStart, 0);
        const effectiveEnd = Math.max(cueEnd, effectiveStart + 0.01);

        // Check if we're in the right time window (with anticipation)
        if (currentAudioTime >= effectiveStart && currentAudioTime <= effectiveEnd) {
          const viseme = corresponding[mouthCue.value];
          if (viseme) {
            // Calculate precise interpolation factor within the cue
            const progress = (currentAudioTime - effectiveStart) / (effectiveEnd - effectiveStart);
            
            // Apply easing function for natural acceleration/deceleration
            const easedProgress = 0.5 - 0.5 * Math.cos(progress * Math.PI);
            
            // Base intensity with natural variation
            const baseIntensity = 0.7 + 0.3 * Math.sin(progress * Math.PI);
            
            // Apply coarticulation effects based on adjacent phonemes
            let intensity = baseIntensity;
            let transitionSpeed = 0.25;
            
            // Adjust for specific viseme types
            switch (mouthCue.value) {
              case 'A': // Open vowels
                intensity = baseIntensity * 1.0;
                transitionSpeed = 0.2;
                break;
              case 'E': // Mid vowels
                intensity = baseIntensity * 0.85;
                transitionSpeed = 0.25;
                break;
              case 'I': // High front vowels
                intensity = baseIntensity * 0.7;
                transitionSpeed = 0.3;
                break;
              case 'O': // Rounded vowels
                intensity = baseIntensity * 0.9;
                transitionSpeed = 0.22;
                break;
              case 'U': // High back vowels
                intensity = baseIntensity * 0.8;
                transitionSpeed = 0.28;
                break;
              case 'B': // Bilabials
                intensity = baseIntensity * 0.95;
                transitionSpeed = 0.15; // Faster for plosives
                break;
              case 'F': // Labiodentals
                intensity = baseIntensity * 0.85;
                transitionSpeed = 0.2;
                break;
              case 'H': // Inter/Aveolars
                intensity = baseIntensity * 0.75;
                transitionSpeed = 0.25;
                break;
              case 'X': // Rest/Silence
                intensity = baseIntensity * 0.3;
                transitionSpeed = 0.3;
                break;
            }
            
            // Apply real-time audio energy boost
            const amplitudeBoost = 0.55 + audioEnergy * 0.9;
            intensity = Math.min(intensity * amplitudeBoost, 1.0);

            // Apply primary viseme
            appliedMorphTargets.add(viseme);
            lerpMorphTarget(viseme, intensity, transitionSpeed);
            
            // Apply coarticulation secondary morph targets
            const coarticulationTargets = getCoarticulationTargets(mouthCue);
            Object.entries(coarticulationTargets).forEach(([target, targetIntensity]) => {
              appliedMorphTargets.add(target);
              lerpMorphTarget(target, intensity * targetIntensity, transitionSpeed * 0.8);
            });
            
            // Add subtle jaw movement for vowel sounds
            if (['A', 'E', 'O', 'U'].includes(mouthCue.value)) {
              lerpMorphTarget('jawOpen', intensity * 0.6, transitionSpeed);
            }
            
            // Add cheek involvement for extreme expressions
            if (intensity > 0.8) {
              lerpMorphTarget('cheekSquintLeft', intensity * 0.3, transitionSpeed);
              lerpMorphTarget('cheekSquintRight', intensity * 0.3, transitionSpeed);
            }
          }
          break;
        }
        
        // Handle transitions between cues
        if (i < lipsync.mouthCues.length - 1) {
          const nextCue = lipsync.mouthCues[i + 1];
          const currentEnd = (mouthCue.end ?? 0) * durationScale;
          const nextStart = (nextCue.start ?? 0) * durationScale;
          if (currentAudioTime > currentEnd && currentAudioTime < nextStart) {
            // In transition period - blend between cues
            const transitionProgress = (currentAudioTime - currentEnd) / Math.max(nextStart - currentEnd, 0.01);
            
            const currentViseme = corresponding[mouthCue.value];
            const nextViseme = corresponding[nextCue.value];
            
            if (currentViseme && nextViseme) {
              // Smooth transition between visemes
              appliedMorphTargets.add(currentViseme);
              appliedMorphTargets.add(nextViseme);
              
              lerpMorphTarget(currentViseme, 1 - transitionProgress, 0.15);
              lerpMorphTarget(nextViseme, transitionProgress, 0.15);
              
              // Blend coarticulation effects during transition
              const currentCoartTargets = getCoarticulationTargets(mouthCue);
              const nextCoartTargets = getCoarticulationTargets(nextCue);
              
              // Apply blended coarticulation
              const allTargets = new Set([
                ...Object.keys(currentCoartTargets),
                ...Object.keys(nextCoartTargets)
              ]);
              
              allTargets.forEach(target => {
                const currentIntensity = currentCoartTargets[target] || 0;
                const nextIntensity = nextCoartTargets[target] || 0;
                const blendedIntensity = currentIntensity * (1 - transitionProgress) + nextIntensity * transitionProgress;
                
                appliedMorphTargets.add(target);
                lerpMorphTarget(target, blendedIntensity, 0.15);
              });
            }
          }
        }
      }
    }

    // Natural mouth movement during speech (micro-expressions)
    if (currentAudioTime < audioDuration) {
      const microMovementIntensity = Math.max(audioEnergy * 0.65, 0.08 + 0.04 * Math.sin(currentAudioTime * 15));
      lerpMorphTarget('jawOpen', microMovementIntensity * 0.9, 0.18);
      lerpMorphTarget('viseme_PP', microMovementIntensity * 0.7, 0.15);
      if (microMovementIntensity > 0.4) {
        lerpMorphTarget('cheekSquintLeft', microMovementIntensity * 0.25, 0.2);
        lerpMorphTarget('cheekSquintRight', microMovementIntensity * 0.25, 0.2);
      }
    }

    // If analyser energy is high but no viseme active (e.g., heuristic miss), open jaw slightly
    if (appliedMorphTargets.size === 0 && audioEnergy > 0.15) {
      const fallbackIntensity = Math.min(audioEnergy * 0.85, 0.9);
      lerpMorphTarget('jawOpen', fallbackIntensity, 0.2);
      lerpMorphTarget('viseme_PP', fallbackIntensity * 0.65, 0.2);
    }

    // Fade out unused morph targets smoothly
    Object.values(corresponding).forEach((value) => {
      if (!appliedMorphTargets.has(value)) {
        lerpMorphTarget(value, 0, 0.15);
      }
    });
  });

// Helper function to calculate coarticulation targets
const getCoarticulationTargets = (mouthCue) => {
  const targets = {};
  
  // Base coarticulation based on viseme type
  switch (mouthCue.value) {
    case 'A': // AA - Wide open jaw
      targets['viseme_AA'] = 0.8;
      targets['viseme_PP'] = 0.4;
      targets['jawOpen'] = 0.6;
      break;
    case 'E': // EH - Slightly open
      targets['viseme_O'] = 0.6;
      targets['viseme_FF'] = 0.4;
      break;
    case 'I': // IH - Tight
      targets['viseme_I'] = 0.7;
      targets['viseme_kk'] = 0.5;
      break;
    case 'O': // OH - Rounded
      targets['viseme_O'] = 0.9;
      targets['viseme_PP'] = 0.6;
      break;
    case 'U': // UH - Tight rounded
      targets['viseme_U'] = 0.8;
      targets['viseme_kk'] = 0.4;
      break;
    case 'B': // PP - Closed lips
      targets['viseme_PP'] = 1.0;
      targets['viseme_kk'] = 0.3;
      break;
    case 'F': // FF - Upper teeth on lower lip
      targets['viseme_FF'] = 0.9;
      targets['viseme_TH'] = 0.6;
      break;
    case 'H': // TH - Tongue between teeth
      targets['viseme_TH'] = 0.8;
      targets['viseme_FF'] = 0.5;
      break;
    case 'G': // KK - Back of tongue raised
      targets['viseme_kk'] = 0.7;
      targets['viseme_TH'] = 0.4;
      break;
  }
  
  // Influence from previous phoneme
  if (mouthCue.prevPhoneme) {
    // Carryover effects from previous sound
    switch (mouthCue.prevPhoneme) {
      case 'b':
      case 'p':
      case 'm':
        targets['viseme_PP'] = (targets['viseme_PP'] || 0) + 0.2;
        break;
      case 'f':
      case 'v':
        targets['viseme_FF'] = (targets['viseme_FF'] || 0) + 0.2;
        break;
    }
  }
  
  // Influence from next phoneme (anticipatory coarticulation)
  if (mouthCue.nextPhoneme) {
    // Prepare for upcoming sound
    switch (mouthCue.nextPhoneme) {
      case 'b':
      case 'p':
      case 'm':
        targets['viseme_PP'] = (targets['viseme_PP'] || 0) + 0.15;
        break;
      case 'f':
      case 'v':
        targets['viseme_FF'] = (targets['viseme_FF'] || 0) + 0.15;
        break;
    }
  }
  
  return targets;
};

  useControls("FacialExpressions", {
    winkLeft: button(() => {
      setWinkLeft(true);
      setTimeout(() => setWinkLeft(false), 300);
    }),
    winkRight: button(() => {
      setWinkRight(true);
      setTimeout(() => setWinkRight(false), 300);
    }),
    animation: {
      value: animation,
      options: filteredAnimations.map((a) => a.name),
      onChange: (value) => setAnimation(value),
    },
    facialExpression: {
      options: Object.keys(facialExpressions),
      onChange: (value) => setFacialExpression(value),
    },
    enableSetupMode: button(() => {
      setupMode = true;
    }),
    disableSetupMode: button(() => {
      setupMode = false;
    }),
    logMorphTargetValues: button(() => {
      if (!isModelLoaded) return;
      
      try {
        const emotionValues = {};
        Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
          if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
            return; // eyes wink/blink are handled separately
          }
          try {
            const index = nodes.EyeLeft.morphTargetDictionary[key];
            if (index !== undefined) {
              const value = nodes.EyeLeft.morphTargetInfluences[index];
              if (value !== undefined && value > 0.01) {
                emotionValues[key] = value;
              }
            }
          } catch (e) {
            // Skip invalid morph targets
            if (process.env.NODE_ENV === 'development') {
              console.warn(`Error reading morph target ${key}:`, e);
            }
          }
        });
        console.log(JSON.stringify(emotionValues, null, 2));
      } catch (e) {
        console.warn('Error logging morph target values:', e);
      }
    }),
  });

  const [, set] = useControls("MorphTarget", () => {
    // Return an empty object to avoid Leva errors
    // The morph target controls were causing too many issues with dynamic updates
    return {};
  });

  useEffect(() => {
    let blinkTimeout;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

  return (
    <>
      {isModelLoaded ? (
        <group {...props} dispose={null} ref={group}>
          <primitive object={nodes.Hips} />
          <skinnedMesh
            name="Wolf3D_Body"
            geometry={nodes.Wolf3D_Body.geometry}
            material={materials.Wolf3D_Body}
            skeleton={nodes.Wolf3D_Body.skeleton}
          />
          <skinnedMesh
            name="Wolf3D_Outfit_Bottom"
            geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
            material={materials.Wolf3D_Outfit_Bottom}
            skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
          />
          <skinnedMesh
            name="Wolf3D_Outfit_Footwear"
            geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
            material={materials.Wolf3D_Outfit_Footwear}
            skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
          />
          <skinnedMesh
            name="Wolf3D_Outfit_Top"
            geometry={nodes.Wolf3D_Outfit_Top.geometry}
            material={materials.Wolf3D_Outfit_Top}
            skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
          />
          <skinnedMesh
            name="Wolf3D_Hair"
            geometry={nodes.Wolf3D_Hair.geometry}
            material={materials.Wolf3D_Hair}
            skeleton={nodes.Wolf3D_Hair.skeleton}
          />
          <skinnedMesh
            name="EyeLeft"
            geometry={nodes.EyeLeft.geometry}
            material={materials.Wolf3D_Eye}
            skeleton={nodes.EyeLeft.skeleton}
            morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
            morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
          />
          <skinnedMesh
            name="EyeRight"
            geometry={nodes.EyeRight.geometry}
            material={materials.Wolf3D_Eye}
            skeleton={nodes.EyeRight.skeleton}
            morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
            morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
          />
          <skinnedMesh
            name="Wolf3D_Head"
            geometry={nodes.Wolf3D_Head.geometry}
            material={materials.Wolf3D_Skin}
            skeleton={nodes.Wolf3D_Head.skeleton}
            morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
            morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
          />
          <skinnedMesh
            name="Wolf3D_Teeth"
            geometry={nodes.Wolf3D_Teeth.geometry}
            material={materials.Wolf3D_Teeth}
            skeleton={nodes.Wolf3D_Teeth.skeleton}
            morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
            morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
          />
        </group>
      ) : (
        <group {...props} ref={group}>
          {/* Loading placeholder or fallback */}
          <mesh>
            <boxGeometry args={[1, 1, 1]} />
            <meshStandardMaterial color="gray" wireframe />
          </mesh>
        </group>
      )}
    </>
  );
}

useGLTF.preload("/models/64f1a714fe61576b46f27ca2.glb");
useGLTF.preload("/models/animations.glb");